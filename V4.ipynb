{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import config\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,\n",
    "                                               chunk_overlap=200)\n",
    "\n",
    "\n",
    "javelin = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/javelin_manual.pdf\")\n",
    "javelin.extend(loader.load())\n",
    "\n",
    "javelin_docs = text_splitter.split_documents(javelin)\n",
    "\n",
    "vector_db_javelin = Chroma.from_documents(javelin_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./javelin_db/\")\n",
    "vector_db_javelin.persist()\n",
    "\n",
    "#######\n",
    "\n",
    "vector3 = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/vector3_manual.pdf\")\n",
    "vector3.extend(loader.load())\n",
    "\n",
    "\n",
    "vector3_docs = text_splitter.split_documents(vector3)\n",
    "\n",
    "vector_db_vector3 = Chroma.from_documents(vector3_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./vector3_db/\")\n",
    "vector_db_vector3.persist()\n",
    "\n",
    "#######\n",
    "\n",
    "firebird = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/firebird_2024_manual.pdf\")\n",
    "firebird.extend(loader.load())\n",
    "\n",
    "\n",
    "firebird_docs = text_splitter.split_documents(firebird)\n",
    "\n",
    "vector_db_firebird = Chroma.from_documents(firebird_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./firebird_db/\")\n",
    "vector_db_firebird.persist()\n",
    "\n",
    "#######\n",
    "\n",
    "icon = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/icon_manual.pdf\")\n",
    "icon.extend(loader.load())\n",
    "\n",
    "\n",
    "icon_docs = text_splitter.split_documents(icon)\n",
    "\n",
    "vector_db_icon = Chroma.from_documents(icon_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./icon_db/\")\n",
    "vector_db_icon.persist()\n",
    "\n",
    "#######\n",
    "\n",
    "mirage_G4 = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/mirage_G4_manual.pdf\")\n",
    "mirage_G4.extend(loader.load())\n",
    "\n",
    "\n",
    "mirage_G4_docs = text_splitter.split_documents(mirage_G4)\n",
    "\n",
    "vector_db_mirage_G4 = Chroma.from_documents(mirage_G4_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./mirage_G4_db/\")\n",
    "vector_db_mirage_G4.persist()\n",
    "\n",
    "#######\n",
    "\n",
    "wings = []\n",
    "loader = PyMuPDFLoader(\"/teamspace/studios/this_studio/para_chat/manuals/Wings_manual.pdf\")\n",
    "wings.extend(loader.load())\n",
    "\n",
    "\n",
    "wings_docs = text_splitter.split_documents(wings)\n",
    "\n",
    "vector_db_wings = Chroma.from_documents(wings_docs, embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"), persist_directory=\"./wings_db/\")\n",
    "vector_db_wings.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /teamspace/studios/this_studio/para_chat/code/ParaChat_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /teamspace/studios/this_studio/para_chat/code/ParaChat_app.py\n",
    "\n",
    "# installing dependencies\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "import streamlit as st\n",
    "import config\n",
    "\n",
    "\n",
    "# setting the llm model:\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "                temperature=0.2,\n",
    "                model_name=\"gpt-4o\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#@st.cache_resource\n",
    "def load_vector_database(brand):\n",
    "    if brand == \"SunPath Javelin\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/javelin_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "    elif brand == \"UPT Vector 3\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/vector3_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "    elif brand == \"Firebird EVO\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/firebird_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "    elif brand == \"Aerodyne Icon\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/icon_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "    elif brand == \"Mirage G4\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/mirage_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "    elif brand == \"Sunrise Manufacturing Wings\":\n",
    "        return Chroma(\n",
    "                        persist_directory=\"/teamspace/studios/this_studio/wings_db\", \n",
    "                        embedding_function=OpenAIEmbeddings(\n",
    "                        model = \"text-embedding-3-large\")\n",
    "                        )\n",
    "                        \n",
    "\n",
    "\n",
    "\n",
    "#@st.cache_resource\n",
    "def create_retriever(vector_db):\n",
    "    return vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Load the vector database\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title(\"Select your container model\")\n",
    "    brand = st.selectbox(\"\", [\"SunPath Javelin\", \"UPT Vector 3\", \"Firebird EVO\", \"Aerodyne Icon\", \"Mirage G4\", \"Sunrise Manufacturing Wings\"])\n",
    "\n",
    "vector_db = load_vector_database(brand)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = create_retriever(vector_db)\n",
    "\n",
    "\n",
    "# Memory\n",
    "\n",
    "@st.cache_resource\n",
    "def init_memory(_llm):\n",
    "    return ConversationBufferMemory(\n",
    "        llm=llm,\n",
    "        output_key='answer',\n",
    "        memory_key='chat_history',\n",
    "        return_messages=True)\n",
    "memory = init_memory(llm)\n",
    "\n",
    "\n",
    "# prompt\n",
    "\n",
    "template = \"\"\"\n",
    "<s> [INST]\n",
    "You are polite and professional question-answering AI assistant specialized in answering technical questions about skydiving equipment.\n",
    "It is very important that you answer the question specific to each manufacturer and container model.\n",
    "If you are not sure which manufacturer the user is asking about, please ask for it again.\n",
    "The user needs to input the manufacturer and the name of the skydive Container system, if that doesn't happen, you should ask for it.\n",
    "\n",
    "In your response, PLEASE ALWAYS:\n",
    "  (0) Be a detail-oriented reader: read the question and context and understand both before answering\n",
    "  (1) Start your answer with a friendly tone, and reiterate the question so the user is sure you understood it\n",
    "  (2) If the context enables you to answer the question, write a detailed, helpful, and easily understandable answer. If you can't find the answer, respond with an explanation, starting with: \"I couldn't find the answer in the information I have access to\".\n",
    "  (3) Ensure your answer answers the question, is helpful and professional, it only needs to be understandeable by professionals.\n",
    "[/INST]\n",
    "[INST]\n",
    "Answer the following question using the context provided.\n",
    "The question is surrounded by the tags <q> </q>.\n",
    "The context is surrounded by the tags <c> </c>.\n",
    "<q>\n",
    "{question}\n",
    "</q>\n",
    "<c>\n",
    "{context}\n",
    "</c>\n",
    "[/INST]\n",
    "</s>\n",
    "[INST]\n",
    "Helpful Answer:\n",
    "[INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template,\n",
    "                        input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "#chain everything:\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "                                              llm=llm,\n",
    "                                              retriever=retriever,\n",
    "                                              memory=memory,\n",
    "                                              return_source_documents=True,\n",
    "                                              combine_docs_chain_kwargs={\"prompt\": prompt}\n",
    "                                              )\n",
    "\n",
    "\n",
    "\n",
    "##### streamlit ####\n",
    "\n",
    "st.title(\"Ask me about nylon\")\n",
    "\n",
    "\n",
    "# Initialise chat history\n",
    "# Chat history saves the previous messages to be displayed\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# React to user input\n",
    "if prompt := st.chat_input(\"write away..\"):\n",
    "\n",
    "    # Display user message in chat message container\n",
    "    st.chat_message(\"user\").markdown(prompt)\n",
    "\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Begin spinner before answering question so it's there for the duration\n",
    "    with st.spinner(\"Going to the riggingloft for answers...\"):\n",
    "\n",
    "        # send question to chain to get answer\n",
    "        answer = chain(prompt)\n",
    "\n",
    "        # extract answer from dictionary returned by chain\n",
    "        response = answer[\"answer\"]\n",
    "        source_documents = answer[\"source_documents\"]\n",
    "\n",
    "\n",
    "        # Display chatbot response in chat message container\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            st.markdown(answer[\"answer\"])\n",
    "\n",
    "        # Display source documents\n",
    "        with st.expander(\"Source Documents\"):\n",
    "            for doc in source_documents:\n",
    "                st.markdown(f\"**Document: {doc.metadata['source']}**\")\n",
    "                st.markdown(doc.page_content)\n",
    "\n",
    "        # Add assistant response to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
